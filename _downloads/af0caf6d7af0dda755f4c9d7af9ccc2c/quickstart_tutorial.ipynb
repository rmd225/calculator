{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IBBYGKpb45kD"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0qDOePz45kE"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Quickstart\n",
        "==========\n",
        "\n",
        "This section runs through the API for common tasks in machine learning.\n",
        "Refer to the links in each section to dive deeper.\n",
        "\n",
        "Working with data\n",
        "-----------------\n",
        "\n",
        "PyTorch has two [primitives to work with\n",
        "data](https://pytorch.org/docs/stable/data.html):\n",
        "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
        "stores the samples and their corresponding labels, and `DataLoader`\n",
        "wraps an iterable around the `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ankbm5N845kF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hNY1q7445kG"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as\n",
        "[TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
        "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
        "include datasets. For this tutorial, we will be using a TorchVision\n",
        "dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many\n",
        "real-world vision data like CIFAR, COCO ([full list\n",
        "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
        "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
        "includes two arguments: `transform` and `target_transform` to modify the\n",
        "samples and labels respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nlJzzn_K45kG",
        "outputId": "d6633142-4363-4ea1-b582-312342eb9508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 204kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 7.62MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2y1It3y45kG"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "iterable over our dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading. Here we define a batch size of\n",
        "64, i.e. each element in the dataloader iterable will return a batch of\n",
        "64 features and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "akeMtHYR45kG",
        "outputId": "092af665-3f28-42a7-c29e-a4a40b00ed65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDcjrd7J45kH"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1xuyxrv45kH"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9FGz46R45kH"
      },
      "source": [
        "Creating Models\n",
        "===============\n",
        "\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "We define the layers of the network in the `__init__` function and\n",
        "specify how data will pass through the network in the `forward`\n",
        "function. To accelerate operations in the neural network, we move it to\n",
        "the GPU or MPS if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G3P-FpWm45kI",
        "outputId": "2f0776f6-8da9-4310-edd0-31231d13c90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj-yIEnn45kI"
      },
      "source": [
        "Read more about [building neural networks in\n",
        "PyTorch](buildmodel_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiqZvKJ545kI"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhzP0xGs45kI"
      },
      "source": [
        "Optimizing the Model Parameters\n",
        "===============================\n",
        "\n",
        "To train a model, we need a [loss\n",
        "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
        "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ETrsHY3d45kI"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUp5Mq3R45kJ"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training\n",
        "dataset (fed to it in batches), and backpropagates the prediction error\n",
        "to adjust the model\\'s parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8IpQb9vh45kJ"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ZdbtGQ45kJ"
      },
      "source": [
        "We also check the model\\'s performance against the test dataset to\n",
        "ensure it is learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u3C_YjNr45kJ"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqB-jyew45kJ"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*).\n",
        "During each epoch, the model learns parameters to make better\n",
        "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
        "we\\'d like to see the accuracy increase and the loss decrease with every\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1qb8U1H-50J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DfRNkNAO45kJ",
        "outputId": "8dc1d584-3f56-4cf1-ab89-a6d09d23169d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.155299  [   64/60000]\n",
            "loss: 1.164929  [ 6464/60000]\n",
            "loss: 0.979922  [12864/60000]\n",
            "loss: 1.120829  [19264/60000]\n",
            "loss: 0.983572  [25664/60000]\n",
            "loss: 1.021125  [32064/60000]\n",
            "loss: 1.054129  [38464/60000]\n",
            "loss: 0.999708  [44864/60000]\n",
            "loss: 1.038038  [51264/60000]\n",
            "loss: 0.966823  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.982746 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.038210  [   64/60000]\n",
            "loss: 1.068445  [ 6464/60000]\n",
            "loss: 0.868135  [12864/60000]\n",
            "loss: 1.030738  [19264/60000]\n",
            "loss: 0.894167  [25664/60000]\n",
            "loss: 0.927623  [32064/60000]\n",
            "loss: 0.978234  [38464/60000]\n",
            "loss: 0.927060  [44864/60000]\n",
            "loss: 0.958389  [51264/60000]\n",
            "loss: 0.901708  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 0.910469 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.951328  [   64/60000]\n",
            "loss: 1.000977  [ 6464/60000]\n",
            "loss: 0.787645  [12864/60000]\n",
            "loss: 0.966171  [19264/60000]\n",
            "loss: 0.833708  [25664/60000]\n",
            "loss: 0.858590  [32064/60000]\n",
            "loss: 0.924309  [38464/60000]\n",
            "loss: 0.877983  [44864/60000]\n",
            "loss: 0.900274  [51264/60000]\n",
            "loss: 0.853629  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.857896 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.884432  [   64/60000]\n",
            "loss: 0.949698  [ 6464/60000]\n",
            "loss: 0.727248  [12864/60000]\n",
            "loss: 0.917333  [19264/60000]\n",
            "loss: 0.790545  [25664/60000]\n",
            "loss: 0.806075  [32064/60000]\n",
            "loss: 0.883077  [38464/60000]\n",
            "loss: 0.843598  [44864/60000]\n",
            "loss: 0.856385  [51264/60000]\n",
            "loss: 0.815967  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.817775 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.830925  [   64/60000]\n",
            "loss: 0.908004  [ 6464/60000]\n",
            "loss: 0.679758  [12864/60000]\n",
            "loss: 0.878945  [19264/60000]\n",
            "loss: 0.757894  [25664/60000]\n",
            "loss: 0.765415  [32064/60000]\n",
            "loss: 0.849428  [38464/60000]\n",
            "loss: 0.818168  [44864/60000]\n",
            "loss: 0.822233  [51264/60000]\n",
            "loss: 0.785070  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.785787 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.786442  [   64/60000]\n",
            "loss: 0.872446  [ 6464/60000]\n",
            "loss: 0.641128  [12864/60000]\n",
            "loss: 0.847894  [19264/60000]\n",
            "loss: 0.732077  [25664/60000]\n",
            "loss: 0.733406  [32064/60000]\n",
            "loss: 0.820674  [38464/60000]\n",
            "loss: 0.798231  [44864/60000]\n",
            "loss: 0.794729  [51264/60000]\n",
            "loss: 0.758907  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.759279 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.748537  [   64/60000]\n",
            "loss: 0.841047  [ 6464/60000]\n",
            "loss: 0.608685  [12864/60000]\n",
            "loss: 0.822263  [19264/60000]\n",
            "loss: 0.710626  [25664/60000]\n",
            "loss: 0.707566  [32064/60000]\n",
            "loss: 0.795170  [38464/60000]\n",
            "loss: 0.781505  [44864/60000]\n",
            "loss: 0.771998  [51264/60000]\n",
            "loss: 0.736090  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.736489 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.715507  [   64/60000]\n",
            "loss: 0.812633  [ 6464/60000]\n",
            "loss: 0.580739  [12864/60000]\n",
            "loss: 0.800564  [19264/60000]\n",
            "loss: 0.692215  [25664/60000]\n",
            "loss: 0.686073  [32064/60000]\n",
            "loss: 0.771988  [38464/60000]\n",
            "loss: 0.766696  [44864/60000]\n",
            "loss: 0.752467  [51264/60000]\n",
            "loss: 0.715823  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.716284 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.686157  [   64/60000]\n",
            "loss: 0.786485  [ 6464/60000]\n",
            "loss: 0.556279  [12864/60000]\n",
            "loss: 0.781637  [19264/60000]\n",
            "loss: 0.676057  [25664/60000]\n",
            "loss: 0.667856  [32064/60000]\n",
            "loss: 0.750471  [38464/60000]\n",
            "loss: 0.753249  [44864/60000]\n",
            "loss: 0.735454  [51264/60000]\n",
            "loss: 0.697404  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.697983 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.659800  [   64/60000]\n",
            "loss: 0.762256  [ 6464/60000]\n",
            "loss: 0.534658  [12864/60000]\n",
            "loss: 0.764708  [19264/60000]\n",
            "loss: 0.661683  [25664/60000]\n",
            "loss: 0.652201  [32064/60000]\n",
            "loss: 0.730315  [38464/60000]\n",
            "loss: 0.740908  [44864/60000]\n",
            "loss: 0.720453  [51264/60000]\n",
            "loss: 0.680426  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.681207 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10 #increase epochs 5 -> 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki9zFpHd45kJ"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "12odp5ic9Pvl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4K5AlxO45kJ"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why epochs improve accuracy:**\n",
        "\n",
        "Epochs are a fundamental part of the training process for neural networks and other machine learning algorithms. They represent the number of times the entire dataset is passed through the algorithm. The right number of epochs is crucial for the model to learn effectively without overfitting. Therefore, accuracy tends to increase with the number of epochs, as the model continues to refine its understanding of the training data."
      ],
      "metadata": {
        "id": "6rCxsYHL8oyJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgCwcogh45kJ"
      },
      "source": [
        "Saving Models\n",
        "=============\n",
        "\n",
        "A common way to save a model is to serialize the internal state\n",
        "dictionary (containing the model parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XT-ApD3p45kJ",
        "outputId": "c8ee797e-bf86-4685-cdfa-0d03a4973732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POV8spNE45kJ"
      },
      "source": [
        "Loading Models\n",
        "==============\n",
        "\n",
        "The process for loading a model includes re-creating the model structure\n",
        "and loading the state dictionary into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "khroeqQN45kJ",
        "outputId": "885cc2f6-216f-4a82-f343-4c9f82ace05d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L6QTeDz45kJ"
      },
      "source": [
        "This model can now be used to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g1vXCf8g45kJ",
        "outputId": "b820668d-79c7-4cc4-d92d-e286228b05d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yisQq8-R45kJ"
      },
      "source": [
        "Read more about [Saving & Loading your\n",
        "model](saveloadrun_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MyNetwork1\n",
        "\n",
        "class MyNetwork1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 4096),  # Increase from 512 to 2048\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 4096),  # Increase from 512 to 2048\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 10)     # Output size remains 10\n",
        "        )\n",
        "    def forward(self, x):\n",
        "       x = self.flatten(x)\n",
        "       logits = self.linear_relu_stack(x)\n",
        "       return logits\n",
        "\n",
        "modelNew = MyNetwork1().to(device)\n",
        "print(modelNew)\n",
        "\n",
        "\n",
        "#modelNew = MyNetwork1()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(modelNew.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3ZGmXNF69ZBH",
        "outputId": "c31ff1b1-1eea-447f-8d53-2f272e80f6bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNetwork1(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, modelNew, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    modelNew.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = modelNew(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "DeVQd_iAALKE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, modelNew, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    modelNew.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = modelNew(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "5AmAR06LsZBO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 #increase epochs 5 -> 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, modelNew, loss_fn, optimizer)\n",
        "    test(test_dataloader, modelNew, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hpkE7VMD-8dq",
        "outputId": "7fdeb515-122e-417a-f0ac-647af7e33cba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304453  [   64/60000]\n",
            "loss: 2.253438  [ 6464/60000]\n",
            "loss: 2.175266  [12864/60000]\n",
            "loss: 2.157633  [19264/60000]\n",
            "loss: 2.086793  [25664/60000]\n",
            "loss: 1.996415  [32064/60000]\n",
            "loss: 2.003506  [38464/60000]\n",
            "loss: 1.889857  [44864/60000]\n",
            "loss: 1.881375  [51264/60000]\n",
            "loss: 1.769673  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.767478 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.807894  [   64/60000]\n",
            "loss: 1.759069  [ 6464/60000]\n",
            "loss: 1.594991  [12864/60000]\n",
            "loss: 1.649389  [19264/60000]\n",
            "loss: 1.497761  [25664/60000]\n",
            "loss: 1.455394  [32064/60000]\n",
            "loss: 1.460659  [38464/60000]\n",
            "loss: 1.353465  [44864/60000]\n",
            "loss: 1.368220  [51264/60000]\n",
            "loss: 1.255302  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.2%, Avg loss: 1.281102 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.339703  [   64/60000]\n",
            "loss: 1.326911  [ 6464/60000]\n",
            "loss: 1.138635  [12864/60000]\n",
            "loss: 1.262207  [19264/60000]\n",
            "loss: 1.107603  [25664/60000]\n",
            "loss: 1.119112  [32064/60000]\n",
            "loss: 1.144392  [38464/60000]\n",
            "loss: 1.066063  [44864/60000]\n",
            "loss: 1.094948  [51264/60000]\n",
            "loss: 1.014576  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.7%, Avg loss: 1.035011 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.078277  [   64/60000]\n",
            "loss: 1.106425  [ 6464/60000]\n",
            "loss: 0.901817  [12864/60000]\n",
            "loss: 1.071912  [19264/60000]\n",
            "loss: 0.924564  [25664/60000]\n",
            "loss: 0.947391  [32064/60000]\n",
            "loss: 0.994906  [38464/60000]\n",
            "loss: 0.925749  [44864/60000]\n",
            "loss: 0.957735  [51264/60000]\n",
            "loss: 0.898824  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.908767 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.930189  [   64/60000]\n",
            "loss: 0.989731  [ 6464/60000]\n",
            "loss: 0.768972  [12864/60000]\n",
            "loss: 0.967371  [19264/60000]\n",
            "loss: 0.829907  [25664/60000]\n",
            "loss: 0.847662  [32064/60000]\n",
            "loss: 0.910975  [38464/60000]\n",
            "loss: 0.848187  [44864/60000]\n",
            "loss: 0.877553  [51264/60000]\n",
            "loss: 0.830796  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.832744 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.833586  [   64/60000]\n",
            "loss: 0.915547  [ 6464/60000]\n",
            "loss: 0.684224  [12864/60000]\n",
            "loss: 0.901311  [19264/60000]\n",
            "loss: 0.772644  [25664/60000]\n",
            "loss: 0.782234  [32064/60000]\n",
            "loss: 0.854955  [38464/60000]\n",
            "loss: 0.800096  [44864/60000]\n",
            "loss: 0.824435  [51264/60000]\n",
            "loss: 0.783897  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.780449 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.763421  [   64/60000]\n",
            "loss: 0.860402  [ 6464/60000]\n",
            "loss: 0.624680  [12864/60000]\n",
            "loss: 0.854671  [19264/60000]\n",
            "loss: 0.733034  [25664/60000]\n",
            "loss: 0.735319  [32064/60000]\n",
            "loss: 0.812276  [38464/60000]\n",
            "loss: 0.767152  [44864/60000]\n",
            "loss: 0.785935  [51264/60000]\n",
            "loss: 0.747621  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.6%, Avg loss: 0.740843 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.709023  [   64/60000]\n",
            "loss: 0.815312  [ 6464/60000]\n",
            "loss: 0.579871  [12864/60000]\n",
            "loss: 0.818926  [19264/60000]\n",
            "loss: 0.702814  [25664/60000]\n",
            "loss: 0.699677  [32064/60000]\n",
            "loss: 0.776978  [38464/60000]\n",
            "loss: 0.742905  [44864/60000]\n",
            "loss: 0.756334  [51264/60000]\n",
            "loss: 0.717413  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.708923 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.665207  [   64/60000]\n",
            "loss: 0.776546  [ 6464/60000]\n",
            "loss: 0.544667  [12864/60000]\n",
            "loss: 0.790109  [19264/60000]\n",
            "loss: 0.678419  [25664/60000]\n",
            "loss: 0.671603  [32064/60000]\n",
            "loss: 0.746423  [38464/60000]\n",
            "loss: 0.724275  [44864/60000]\n",
            "loss: 0.732951  [51264/60000]\n",
            "loss: 0.691218  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.682238 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.629097  [   64/60000]\n",
            "loss: 0.742517  [ 6464/60000]\n",
            "loss: 0.516109  [12864/60000]\n",
            "loss: 0.766044  [19264/60000]\n",
            "loss: 0.657987  [25664/60000]\n",
            "loss: 0.648734  [32064/60000]\n",
            "loss: 0.719406  [38464/60000]\n",
            "loss: 0.709641  [44864/60000]\n",
            "loss: 0.714307  [51264/60000]\n",
            "loss: 0.668010  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.659459 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see a small change in the accruacy by making the hidden layer larger. I increased it from 512 to 4096 (8x fold), yet the accuracy at epoch 10 only increased by 1-2%. Perhaps this is because NN  can already learn most of the dataset's complexity when the hidden layers are sufficiently large (EX, 512 units).\n"
      ],
      "metadata": {
        "id": "NXCJIIjeDk_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MyNetwork2:**"
      ],
      "metadata": {
        "id": "9Xb-3cXxFgOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MyNetwork2\n",
        "\n",
        "class MyNetwork2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "             nn.Linear(28*28, 4096),  # Input layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 4096),    # Hidden layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 4096),    # Hidden layer 2\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(512, 512),    # Hidden layer 3\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(512, 512),    # Hidden layer 4\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(512, 512),    # Hidden layer 4\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(4096, 10)      # Output layer\n",
        "        )\n",
        "    def forward(self, x):\n",
        "       x = self.flatten(x)\n",
        "       logits = self.linear_relu_stack(x)\n",
        "       return logits\n",
        "\n",
        "modelNew2 = MyNetwork2().to(device)\n",
        "print(modelNew2)\n",
        "\n",
        "\n",
        "#modelNew = MyNetwork1()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(modelNew2.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xIo1etBnE5bV",
        "outputId": "7301e0c1-0099-4c7d-cbc7-ef1846aca362"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNetwork2(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, modelNew2, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    modelNew2.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = modelNew2(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "p7yDzLGgFUHf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, modelNew2, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    modelNew2.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = modelNew2(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "ShDdre7drlSS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30 #increase epochs 10 -> 15\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, modelNew2, loss_fn, optimizer)\n",
        "    test(test_dataloader, modelNew2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SXCDTkONFacE",
        "outputId": "d4b94abf-9335-4e83-86ca-25b031436c97"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298455  [   64/60000]\n",
            "loss: 2.288244  [ 6464/60000]\n",
            "loss: 2.268312  [12864/60000]\n",
            "loss: 2.265350  [19264/60000]\n",
            "loss: 2.244666  [25664/60000]\n",
            "loss: 2.215292  [32064/60000]\n",
            "loss: 2.225639  [38464/60000]\n",
            "loss: 2.189424  [44864/60000]\n",
            "loss: 2.189457  [51264/60000]\n",
            "loss: 2.156323  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.2%, Avg loss: 2.148527 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.163759  [   64/60000]\n",
            "loss: 2.151938  [ 6464/60000]\n",
            "loss: 2.088949  [12864/60000]\n",
            "loss: 2.104005  [19264/60000]\n",
            "loss: 2.047206  [25664/60000]\n",
            "loss: 1.986835  [32064/60000]\n",
            "loss: 2.011130  [38464/60000]\n",
            "loss: 1.924229  [44864/60000]\n",
            "loss: 1.926486  [51264/60000]\n",
            "loss: 1.841476  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 1.837495 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.882484  [   64/60000]\n",
            "loss: 1.841889  [ 6464/60000]\n",
            "loss: 1.703014  [12864/60000]\n",
            "loss: 1.739555  [19264/60000]\n",
            "loss: 1.611939  [25664/60000]\n",
            "loss: 1.572712  [32064/60000]\n",
            "loss: 1.581740  [38464/60000]\n",
            "loss: 1.478020  [44864/60000]\n",
            "loss: 1.496501  [51264/60000]\n",
            "loss: 1.388007  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.0%, Avg loss: 1.404797 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.482123  [   64/60000]\n",
            "loss: 1.449226  [ 6464/60000]\n",
            "loss: 1.278908  [12864/60000]\n",
            "loss: 1.359122  [19264/60000]\n",
            "loss: 1.227523  [25664/60000]\n",
            "loss: 1.240733  [32064/60000]\n",
            "loss: 1.254757  [38464/60000]\n",
            "loss: 1.174869  [44864/60000]\n",
            "loss: 1.205554  [51264/60000]\n",
            "loss: 1.124730  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.9%, Avg loss: 1.138379 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.210024  [   64/60000]\n",
            "loss: 1.210757  [ 6464/60000]\n",
            "loss: 1.021645  [12864/60000]\n",
            "loss: 1.146437  [19264/60000]\n",
            "loss: 1.010610  [25664/60000]\n",
            "loss: 1.043599  [32064/60000]\n",
            "loss: 1.079092  [38464/60000]\n",
            "loss: 1.004902  [44864/60000]\n",
            "loss: 1.041007  [51264/60000]\n",
            "loss: 0.982144  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.987714 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.041275  [   64/60000]\n",
            "loss: 1.075976  [ 6464/60000]\n",
            "loss: 0.864070  [12864/60000]\n",
            "loss: 1.020748  [19264/60000]\n",
            "loss: 0.889037  [25664/60000]\n",
            "loss: 0.917080  [32064/60000]\n",
            "loss: 0.976816  [38464/60000]\n",
            "loss: 0.905536  [44864/60000]\n",
            "loss: 0.936994  [51264/60000]\n",
            "loss: 0.895467  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.894874 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.926796  [   64/60000]\n",
            "loss: 0.990798  [ 6464/60000]\n",
            "loss: 0.760622  [12864/60000]\n",
            "loss: 0.938249  [19264/60000]\n",
            "loss: 0.816348  [25664/60000]\n",
            "loss: 0.830868  [32064/60000]\n",
            "loss: 0.910594  [38464/60000]\n",
            "loss: 0.845253  [44864/60000]\n",
            "loss: 0.867237  [51264/60000]\n",
            "loss: 0.836544  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.832581 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.844155  [   64/60000]\n",
            "loss: 0.929134  [ 6464/60000]\n",
            "loss: 0.687970  [12864/60000]\n",
            "loss: 0.879692  [19264/60000]\n",
            "loss: 0.768578  [25664/60000]\n",
            "loss: 0.769898  [32064/60000]\n",
            "loss: 0.862352  [38464/60000]\n",
            "loss: 0.805942  [44864/60000]\n",
            "loss: 0.818188  [51264/60000]\n",
            "loss: 0.792660  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.787220 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.780784  [   64/60000]\n",
            "loss: 0.879353  [ 6464/60000]\n",
            "loss: 0.633629  [12864/60000]\n",
            "loss: 0.835820  [19264/60000]\n",
            "loss: 0.733902  [25664/60000]\n",
            "loss: 0.725424  [32064/60000]\n",
            "loss: 0.823595  [38464/60000]\n",
            "loss: 0.777718  [44864/60000]\n",
            "loss: 0.781821  [51264/60000]\n",
            "loss: 0.757527  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.751664 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.729627  [   64/60000]\n",
            "loss: 0.836373  [ 6464/60000]\n",
            "loss: 0.591064  [12864/60000]\n",
            "loss: 0.801634  [19264/60000]\n",
            "loss: 0.706681  [25664/60000]\n",
            "loss: 0.691840  [32064/60000]\n",
            "loss: 0.790044  [38464/60000]\n",
            "loss: 0.755153  [44864/60000]\n",
            "loss: 0.753256  [51264/60000]\n",
            "loss: 0.727870  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.722042 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.686743  [   64/60000]\n",
            "loss: 0.798136  [ 6464/60000]\n",
            "loss: 0.556191  [12864/60000]\n",
            "loss: 0.773894  [19264/60000]\n",
            "loss: 0.684217  [25664/60000]\n",
            "loss: 0.665542  [32064/60000]\n",
            "loss: 0.759793  [38464/60000]\n",
            "loss: 0.735911  [44864/60000]\n",
            "loss: 0.729869  [51264/60000]\n",
            "loss: 0.701916  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.696265 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.649781  [   64/60000]\n",
            "loss: 0.763764  [ 6464/60000]\n",
            "loss: 0.526844  [12864/60000]\n",
            "loss: 0.750615  [19264/60000]\n",
            "loss: 0.665150  [25664/60000]\n",
            "loss: 0.644279  [32064/60000]\n",
            "loss: 0.731974  [38464/60000]\n",
            "loss: 0.718885  [44864/60000]\n",
            "loss: 0.710176  [51264/60000]\n",
            "loss: 0.678631  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.673288 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.617340  [   64/60000]\n",
            "loss: 0.732882  [ 6464/60000]\n",
            "loss: 0.501715  [12864/60000]\n",
            "loss: 0.730484  [19264/60000]\n",
            "loss: 0.648835  [25664/60000]\n",
            "loss: 0.626774  [32064/60000]\n",
            "loss: 0.706200  [38464/60000]\n",
            "loss: 0.703812  [44864/60000]\n",
            "loss: 0.693475  [51264/60000]\n",
            "loss: 0.657452  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.652660 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.588733  [   64/60000]\n",
            "loss: 0.705275  [ 6464/60000]\n",
            "loss: 0.479907  [12864/60000]\n",
            "loss: 0.712717  [19264/60000]\n",
            "loss: 0.635008  [25664/60000]\n",
            "loss: 0.612040  [32064/60000]\n",
            "loss: 0.682482  [38464/60000]\n",
            "loss: 0.690822  [44864/60000]\n",
            "loss: 0.679530  [51264/60000]\n",
            "loss: 0.638231  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.634156 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.563478  [   64/60000]\n",
            "loss: 0.680710  [ 6464/60000]\n",
            "loss: 0.460905  [12864/60000]\n",
            "loss: 0.696803  [19264/60000]\n",
            "loss: 0.623248  [25664/60000]\n",
            "loss: 0.599529  [32064/60000]\n",
            "loss: 0.660747  [38464/60000]\n",
            "loss: 0.679943  [44864/60000]\n",
            "loss: 0.668050  [51264/60000]\n",
            "loss: 0.620677  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.617625 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.541150  [   64/60000]\n",
            "loss: 0.658880  [ 6464/60000]\n",
            "loss: 0.444311  [12864/60000]\n",
            "loss: 0.682520  [19264/60000]\n",
            "loss: 0.613200  [25664/60000]\n",
            "loss: 0.588741  [32064/60000]\n",
            "loss: 0.640966  [38464/60000]\n",
            "loss: 0.671271  [44864/60000]\n",
            "loss: 0.658662  [51264/60000]\n",
            "loss: 0.604511  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.602906 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.521304  [   64/60000]\n",
            "loss: 0.639565  [ 6464/60000]\n",
            "loss: 0.429762  [12864/60000]\n",
            "loss: 0.669623  [19264/60000]\n",
            "loss: 0.604458  [25664/60000]\n",
            "loss: 0.579242  [32064/60000]\n",
            "loss: 0.623123  [38464/60000]\n",
            "loss: 0.664710  [44864/60000]\n",
            "loss: 0.651126  [51264/60000]\n",
            "loss: 0.589506  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.589816 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.503569  [   64/60000]\n",
            "loss: 0.622477  [ 6464/60000]\n",
            "loss: 0.416895  [12864/60000]\n",
            "loss: 0.657783  [19264/60000]\n",
            "loss: 0.596523  [25664/60000]\n",
            "loss: 0.570771  [32064/60000]\n",
            "loss: 0.607112  [38464/60000]\n",
            "loss: 0.660008  [44864/60000]\n",
            "loss: 0.645029  [51264/60000]\n",
            "loss: 0.575440  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.578166 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.487557  [   64/60000]\n",
            "loss: 0.607348  [ 6464/60000]\n",
            "loss: 0.405422  [12864/60000]\n",
            "loss: 0.646887  [19264/60000]\n",
            "loss: 0.589085  [25664/60000]\n",
            "loss: 0.563024  [32064/60000]\n",
            "loss: 0.592758  [38464/60000]\n",
            "loss: 0.656961  [44864/60000]\n",
            "loss: 0.640073  [51264/60000]\n",
            "loss: 0.562104  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.567756 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.472982  [   64/60000]\n",
            "loss: 0.593869  [ 6464/60000]\n",
            "loss: 0.395113  [12864/60000]\n",
            "loss: 0.636797  [19264/60000]\n",
            "loss: 0.581998  [25664/60000]\n",
            "loss: 0.555792  [32064/60000]\n",
            "loss: 0.579910  [38464/60000]\n",
            "loss: 0.655274  [44864/60000]\n",
            "loss: 0.636034  [51264/60000]\n",
            "loss: 0.549437  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.558416 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.459579  [   64/60000]\n",
            "loss: 0.581822  [ 6464/60000]\n",
            "loss: 0.385737  [12864/60000]\n",
            "loss: 0.627381  [19264/60000]\n",
            "loss: 0.575069  [25664/60000]\n",
            "loss: 0.548874  [32064/60000]\n",
            "loss: 0.568378  [38464/60000]\n",
            "loss: 0.654633  [44864/60000]\n",
            "loss: 0.632626  [51264/60000]\n",
            "loss: 0.537367  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.549993 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.447173  [   64/60000]\n",
            "loss: 0.570967  [ 6464/60000]\n",
            "loss: 0.377149  [12864/60000]\n",
            "loss: 0.618561  [19264/60000]\n",
            "loss: 0.568172  [25664/60000]\n",
            "loss: 0.542151  [32064/60000]\n",
            "loss: 0.558087  [38464/60000]\n",
            "loss: 0.654801  [44864/60000]\n",
            "loss: 0.629683  [51264/60000]\n",
            "loss: 0.525860  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.542364 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.435655  [   64/60000]\n",
            "loss: 0.561225  [ 6464/60000]\n",
            "loss: 0.369279  [12864/60000]\n",
            "loss: 0.610232  [19264/60000]\n",
            "loss: 0.561275  [25664/60000]\n",
            "loss: 0.535557  [32064/60000]\n",
            "loss: 0.548784  [38464/60000]\n",
            "loss: 0.655543  [44864/60000]\n",
            "loss: 0.627094  [51264/60000]\n",
            "loss: 0.514858  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.535424 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.424892  [   64/60000]\n",
            "loss: 0.552449  [ 6464/60000]\n",
            "loss: 0.362020  [12864/60000]\n",
            "loss: 0.602385  [19264/60000]\n",
            "loss: 0.554365  [25664/60000]\n",
            "loss: 0.529059  [32064/60000]\n",
            "loss: 0.540426  [38464/60000]\n",
            "loss: 0.656717  [44864/60000]\n",
            "loss: 0.624693  [51264/60000]\n",
            "loss: 0.504339  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.529079 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.414842  [   64/60000]\n",
            "loss: 0.544531  [ 6464/60000]\n",
            "loss: 0.355325  [12864/60000]\n",
            "loss: 0.594954  [19264/60000]\n",
            "loss: 0.547472  [25664/60000]\n",
            "loss: 0.522585  [32064/60000]\n",
            "loss: 0.532852  [38464/60000]\n",
            "loss: 0.658099  [44864/60000]\n",
            "loss: 0.622341  [51264/60000]\n",
            "loss: 0.494308  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.523252 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.405384  [   64/60000]\n",
            "loss: 0.537369  [ 6464/60000]\n",
            "loss: 0.349072  [12864/60000]\n",
            "loss: 0.587874  [19264/60000]\n",
            "loss: 0.540636  [25664/60000]\n",
            "loss: 0.516181  [32064/60000]\n",
            "loss: 0.525901  [38464/60000]\n",
            "loss: 0.659526  [44864/60000]\n",
            "loss: 0.619946  [51264/60000]\n",
            "loss: 0.484818  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.517893 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.396505  [   64/60000]\n",
            "loss: 0.530883  [ 6464/60000]\n",
            "loss: 0.343256  [12864/60000]\n",
            "loss: 0.581114  [19264/60000]\n",
            "loss: 0.533870  [25664/60000]\n",
            "loss: 0.509935  [32064/60000]\n",
            "loss: 0.519519  [38464/60000]\n",
            "loss: 0.660885  [44864/60000]\n",
            "loss: 0.617487  [51264/60000]\n",
            "loss: 0.475881  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.512942 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.388148  [   64/60000]\n",
            "loss: 0.524963  [ 6464/60000]\n",
            "loss: 0.337825  [12864/60000]\n",
            "loss: 0.574673  [19264/60000]\n",
            "loss: 0.527180  [25664/60000]\n",
            "loss: 0.503839  [32064/60000]\n",
            "loss: 0.513625  [38464/60000]\n",
            "loss: 0.662063  [44864/60000]\n",
            "loss: 0.614930  [51264/60000]\n",
            "loss: 0.467539  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.508363 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.380269  [   64/60000]\n",
            "loss: 0.519518  [ 6464/60000]\n",
            "loss: 0.332744  [12864/60000]\n",
            "loss: 0.568532  [19264/60000]\n",
            "loss: 0.520633  [25664/60000]\n",
            "loss: 0.497907  [32064/60000]\n",
            "loss: 0.508195  [38464/60000]\n",
            "loss: 0.662953  [44864/60000]\n",
            "loss: 0.612300  [51264/60000]\n",
            "loss: 0.459696  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.504107 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.372784  [   64/60000]\n",
            "loss: 0.514526  [ 6464/60000]\n",
            "loss: 0.327961  [12864/60000]\n",
            "loss: 0.562663  [19264/60000]\n",
            "loss: 0.514253  [25664/60000]\n",
            "loss: 0.492183  [32064/60000]\n",
            "loss: 0.503107  [38464/60000]\n",
            "loss: 0.663544  [44864/60000]\n",
            "loss: 0.609591  [51264/60000]\n",
            "loss: 0.452428  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.500136 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def find_and_display_errors(modelNew2, dataloader, device):\n",
        "    modelNew2.eval()  #set the model to evaluation mode\n",
        "    errors_found = 0\n",
        "\n",
        "    #iterate through the dataloader\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = modelNew2(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)  #predicted class\n",
        "\n",
        "        #find errors\n",
        "        mismatches = predictions != labels\n",
        "        if mismatches.any():\n",
        "            for i in range(len(inputs)):\n",
        "                if mismatches[i]:\n",
        "                    img = inputs[i] #display image, ground truth, and prediction\n",
        "                    ground_truth = labels[i].item()\n",
        "                    prediction = predictions[i].item()\n",
        "\n",
        "                    #convert the image and display\n",
        "                    image_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "                    plt.imshow(image_np)\n",
        "                    plt.axis('off')\n",
        "                    plt.title(f\"Ground Truth: {ground_truth}, Prediction: {prediction}\")\n",
        "                    plt.show()\n",
        "\n",
        "                    errors_found += 1\n",
        "                    if errors_found >= 3:  #find 3 errors\n",
        "                        return\n",
        "#test on modelNew2\n",
        "find_and_display_errors(modelNew2, test_dataloader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1250
        },
        "id": "9RnWOYo36M0V",
        "outputId": "2dd6ace4-382b-462a-eb5e-b7f5b0a68e31"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaY0lEQVR4nO3deXSU9b3H8c9kJxACgbBFCDuCC7RYsBQKFISWxbKJgJYgpD1gK6CgB6UeoWLVSoFyKojHsohyWq215VC9IIuX2w16bcUqoiRNUkENYTUkAbL87h/cfHWYQOb3GBbb9+sc/2DyfOd5ZkneeWYmP0POOScAACTFXO4DAABcOYgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCJEmhUEgLFiy43IdxQVOmTFGDBg0u92F8Iaxdu1ahUEj5+fl22YABAzRgwIA628eCBQsUCoXq7PpwZSAKHvLy8vSDH/xAnTt3VnJyspKTk9WtWzd9//vf11tvvXW5D++iGjBggEKhUK3/fd6wlJaWasGCBXr99dfr5Lhr8/rrr1/w9jzyyCOBrrdt27Zh19OsWTP169dPL7/8ch3fgovrUj8en0d+fv55H8df/vKXl/vwvjDiLvcBfFFs2rRJt956q+Li4nTbbbepe/fuiomJ0b59+/Sb3/xGK1euVF5enjIzMy/3oV4U8+fPV3Z2tv37r3/9q5YvX64HHnhAXbt2tcuvv/76z7Wf0tJSLVy4UJLq9Lfa8+natavWr18fcfn69eu1ZcsWDRkyJPB19+jRQ3PmzJEkffjhh1q1apXGjBmjlStXavr06YGvN6gtW7Z4z1zo8fjhD3+oefPm1cWh1amJEydq2LBhYZd99atfvUxH88VDFKKQm5urCRMmKDMzU9u2bVPLli3Dvv74449rxYoViom58IlXSUmJ6tevfzEP9aK56aabwv6dlJSk5cuX66abbrrgD+8r/TY3b95ct99+e8TlCxcuVKdOnfSVr3wl8HVnZGSEXffkyZPVsWNHLV269LxRqKioUFVVlRISEgLv93zq+jrj4uIUF3fl/Qj58pe/XONjiujw8lEUfvKTn6ikpERr1qyJCIJ09ptj5syZat26tV1W/fp3bm6uhg0bppSUFN12222Szv6gnDNnjlq3bq3ExER16dJFixcv1mcXrK0+FV67dm3E/s59mab6td2cnBxNmTJFjRo1Umpqqu644w6VlpaGzZ4+fVp333230tPTlZKSoptvvlkHDhz4nPdQ+HHs3btXkyZNUuPGjdW3b19J5389e8qUKWrbtq3d5vT0dElnfyif7yWpgwcPatSoUWrQoIHS09M1d+5cVVZWhm3z0Ucfad++fSovL/e+Hbt371ZOTo49XnWlRYsW6tq1q/Ly8iR9+hgvXrxYy5YtU4cOHZSYmKi9e/dKkvbt26dx48YpLS1NSUlJuuGGG7Rx48aI633nnXf0jW98Q/Xq1dNVV12lRYsWqaqqKmK7mh6DU6dOacGCBercubOSkpLUsmVLjRkzRrm5ubU+HjW9p1BRUaGHH37Ybkvbtm31wAMP6PTp02HbtW3bViNGjNAf/vAH9erVS0lJSWrfvr2effbZiOPOzc1Vbm5udHfy/yspKdGZM2e8ZnDWlZf5K9CmTZvUsWNH9e7d22uuoqJCQ4cOVd++fbV48WIlJyfLOaebb75ZO3bs0LRp09SjRw9t3rxZ9957rw4ePKilS5cGPs7x48erXbt2evTRR/W3v/1NzzzzjJo1a6bHH3/ctsnOztZzzz2nSZMmqU+fPtq+fbuGDx8eeJ81ueWWW9SpUyf9+Mc/ls/K7Onp6Vq5cqVmzJih0aNHa8yYMZLCX5KqrKzU0KFD1bt3by1evFhbt27VT3/6U3Xo0EEzZsyw7e6//36tW7dOeXl5Fp1oPf/885JU51EoLy/XBx98oCZNmoRdvmbNGp06dUrf+973lJiYqLS0NL3zzjv62te+poyMDM2bN0/169fXCy+8oFGjRumll17S6NGjJUkff/yxBg4cqIqKCtvu6aefVr169Wo9nsrKSo0YMULbtm3ThAkTNGvWLBUXF+u1117T22+/rcGDB9f6eJwrOztb69at07hx4zRnzhzt2rVLjz76qN59992I91NycnI0btw4TZs2TVlZWVq9erWmTJminj176pprrrHtBg0aJElhb5pfyMKFC3XvvfcqFAqpZ8+eeuSRRz7Xy4D/cRwu6MSJE06SGzVqVMTXjh075oqKiuy/0tJS+1pWVpaT5ObNmxc289vf/tZJcosWLQq7fNy4cS4UCrmcnBznnHN5eXlOkluzZk3EfiW5hx56yP790EMPOUlu6tSpYduNHj3aNWnSxP795ptvOknuzjvvDNtu0qRJEddZmxdffNFJcjt27Ig4jokTJ0Zs379/f9e/f/+Iy7OyslxmZqb9u6io6LzHUn2f/uhHPwq7/Etf+pLr2bNnjdvm5eVFfZucc66iosI1b97c9erVy2vuXJmZmW7IkCH23NizZ4+bMGGCk+Tuuusu59ynj3HDhg3doUOHwuYHDRrkrrvuOnfq1Cm7rKqqyvXp08d16tTJLps9e7aT5Hbt2mWXHTp0yKWmpkbc/nMfg9WrVztJbsmSJRHHX1VV5Zy78ONR/XhXq35+ZWdnh203d+5cJ8lt37497P6R5Hbu3Bl23ImJiW7OnDkR9+VnnyPnU1BQ4IYMGeJWrlzpNm7c6JYtW+batGnjYmJi3KZNm2qdx1m8fFSLTz75RJJq/CjkgAEDlJ6ebv89+eSTEdt89rdXSXrllVcUGxurmTNnhl0+Z84cOef06quvBj7Wc1+n7tevn44cOWK34ZVXXpGkiH3Pnj078D6jOY66VtPt/Oc//xl22dq1a+Wc8z5L2LZtmwoLC+vkLGHLli323OjevbtefPFFfec73wk7c5OksWPH2ss0knT06FFt375d48ePV3FxsQ4fPqzDhw/ryJEjGjp0qPbv36+DBw9KOvuY3njjjerVq5fNp6enR3X8L730kpo2baq77ror4mtBPmpa/fy65557wi6vfrP997//fdjl3bp1U79+/cKOu0uXLhGPZX5+flRnCW3atNHmzZs1ffp0jRw5UrNmzdLf//53paen2zGgdrx8VIuUlBRJ0smTJyO+tmrVKhUXF6uwsLDGN7bi4uJ01VVXhV1WUFCgVq1a2fVWq/4ET0FBQeBjbdOmTdi/GzduLEk6duyYGjZsqIKCAsXExKhDhw5h23Xp0iXwPmvSrl27Or2+z0pKSgr7ASqdvZ3Hjh2rk+t//vnnFRsbq1tvvfVzX1fv3r21aNEihUIhJScnq2vXrmrUqFHEdufeXzk5OXLO6cEHH9SDDz5Y43UfOnRIGRkZKigoqPFlzWge09zcXHXp0qXO3iyufn517Ngx7PIWLVqoUaNGEc/tc5+vUt0+lpKUlpamO+64Q4899pgOHDgQ8f2ISEShFqmpqWrZsqXefvvtiK9VfzOe77eYxMTEWj+RdD7n+03t3DdUPys2NrbGy90l/j+u1vR6digUqvE4LnR7anK+21gXysrK9PLLL2vw4MFq3rz5576+pk2bavDgwbVud+79Vf0m8dy5czV06NAaZ879wXslifYs41I9X6s/AHL06FGiEAWiEIXhw4frmWee0e7du8NO04PIzMzU1q1bVVxcHHa2sG/fPvu69Olv+cePHw+b/zxnEpmZmaqqqrLfEKu99957ga8zWo0bN454WUCKvD2X8y9kN27cqOLi4jp/g9lX+/btJUnx8fG1RiUzM1P79++PuDyax7RDhw7atWuXysvLFR8fX+M2Po9H9fNr//79YX+7UlhYqOPHj1+2v+Gpft6de4aJmvGeQhTuu+8+JScna+rUqSosLIz4us9vNsOGDVNlZaV+/vOfh12+dOlShUIhfetb35IkNWzYUE2bNtXOnTvDtluxYkWAW3BW9XUvX7487PJly5YFvs5odejQQfv27VNRUZFdtmfPHv3xj38M2y45OVlSZAx9BflI6oYNG5ScnGyf7LlcmjVrpgEDBmjVqlX66KOPIr7+2ftw2LBh+stf/qLdu3eHfb36E1QXMnbsWB0+fDjiuSh9+pz2eTyq/2Ds3OfTkiVLJCnwp9yi/UjqZ++XagcPHtTq1at1/fXX1/hxckTiTCEKnTp10oYNGzRx4kR16dLF/qLZOae8vDxt2LBBMTExUZ2ajhw5UgMHDtT8+fOVn5+v7t27a8uWLfrd736n2bNnh73en52drccee0zZ2dm64YYbtHPnTr3//vuBb0ePHj00ceJErVixQidOnFCfPn20bds25eTkBL7OaE2dOlVLlizR0KFDNW3aNB06dEhPPfWUrrnmGnsjXDr7Ukq3bt30q1/9Sp07d1ZaWpquvfZaXXvttV778/1I6tGjR/Xqq69q7Nix511fKT8/X+3atVNWVlaNfz9Sl5588kn17dtX1113nb773e+qffv2Kiws1J///GcdOHBAe/bskXT2F5b169frm9/8pmbNmmUfSc3MzKx16ZXJkyfr2Wef1T333KPdu3erX79+Kikp0datW3XnnXfq29/+ttfj0b17d2VlZenpp5/W8ePH1b9/f+3evVvr1q3TqFGjNHDgwED3RbQfSb3vvvuUm5urQYMGqVWrVsrPz9eqVatUUlKin/3sZ4H2/R/psn3u6QsoJyfHzZgxw3Xs2NElJSW5evXquauvvtpNnz7dvfnmm2HbZmVlufr169d4PcXFxe7uu+92rVq1cvHx8a5Tp07uiSeesI8BVistLXXTpk1zqampLiUlxY0fP94dOnTovB9JLSoqCptfs2ZNxMcSy8rK3MyZM12TJk1c/fr13ciRI90HH3xQpx9JPfc4qj333HOuffv2LiEhwfXo0cNt3rw54iOpzjn3pz/9yfXs2dMlJCSEHdf57tNzPxpZve25t/1CnnrqKSfJbdy48bzb/OMf/6jxY8Y1yczMdMOHD7/gNtUfSX3iiSdq/Hpubq6bPHmya9GihYuPj3cZGRluxIgR7te//nXYdm+99Zbr37+/S0pKchkZGe7hhx92v/jFL2r9SKpzZ59j8+fPd+3atXPx8fGuRYsWbty4cS43N9e2Od/jUdP9Xl5e7hYuXGjX17p1a3f//feHfbT2QvdPTccY7UdSN2zY4L7+9a+79PR0FxcX55o2bepGjx7t3njjjVpn8amQc5f4XUjgC2rFihX222hdvBENXIl4TwGI0o4dOzRz5kyCgH9rnCkAAAxnCgAAQxQAAIYoAAAMUQAAmKj/eO2mmFsu5nEAAC6y16perHUbzhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwMRd7gPAf44DD/QJNHeqW5n3TMfb/x5oX5dEKBRszrm6PQ6gBpwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgLuqCeDEpKd4zVcXFF+FIahbk+FyZ/+Jsl5KrqLgk+zlx+43eM2UtKwPtK1SU6D3z4cvdvGdazzvjPVP5Xo73zKVc2C4Ud2nWvLxUzztcfJwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgLupqWcdfaOY9E7+qa6B91fvtbu+ZS7n43r+btGn/8p4perNNoH3FnAl5z5R86L/YYe53/H9HSiryf45nPLfPe0aSKo8c9Z4JslBdoEX0Qv6P0RXvEi5ceCXhTAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAAAm6uUQYxulel9572YF3jO/G5bmPSNJLZJv9J5p/MZh/x0VFnmPVJ74xHsmFBvrPSNJoas7es+8991G3jOJh0v8Z44E+x3k6iH7vWfe+69O3jNnUv1XxUy4yf85tK+z/7FJUv18/+dE69/7H1/l3ve9Z650gVZ+DfI9WFnpP6Ngq9leLJwpAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgol4lqqx3kEW83vSeiG1QHmA/Uun4k94zR65N956JPdPMeybmjPeIStsHGJL0lavzvGcGxh3wntnx12u8ZxK7F3vPSNLe1/0X+Uss899PTHnIe+bMtqbeM3Hp/gvvSdLJDv6LpuU+lOS/o/1f9R5J2+t/m9L+x/95J0kVH/jPBVpw7gpapO5S4kwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAAAT9YJ4pxtHvam5Ltl/4aqNFdd7z0hSixT/xda6Dt3vPfP+J/4L4jWIP+098+HJVO8ZSfrfN/wXLmyY4/+7QWqV94hCexv4D0k61cR/obqKev77STzmv6jb6cb+xxZX6j9zds7/e1AH/O/z8hT/+6Hwa/4zRT2v8p6RpNiy1t4zCSf87/OGBf5P8uSP/L/XJSnujfe8Z6pKSwPtqzacKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYKJeYSuhuNL7yqelfuw98/iHid4zkpT/L/9FsvY3zvCecYn+90PsCf+FzGJOB1s0LSbRf2GyE9dUBNqXr9iSYL+DxJb5z1TF+8+UNfe/72LO+O8nvjjYY1uV4H98QRa3Szzi/zglHI/1ngmqKsCPiFNN/e+Hshb++4ktS/IfkpT2QEvvmcpf+S/OGQ3OFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMFGv1FYw0n8Rr21l/otkVbQIsMKYpHopp7xnKvNTvGcSDyR4z7gAa4Wdaum/8J4kufgq/6E4/8XCgqiMC3BskipTAiwgV+k/k1jk/0DFnPHfT3yx98j/899XXKn/TL1D/s+HIM/xyoRgCwM6/2/1QAtMlvv/eNCZFuX+Q5Kc8z++66b/I9C+asOZAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAEzUq6RmtD3sfeUvHOnlPZP8bqL3jCSVdvWfiWtV5j1zuqX/aoaVpVHfzZ+6hLkOxfivihmT4L+Ka3x8sJVfq6r87/OKcv9lO12a//KbpwPspyzI80EKtPJrEKeb+D/5XIDna1X9Cv8hSQmpp71nGqeUes+cKKnnPdMgMdgqzyOuett7pmvSh4H2VRvOFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMFGvzFV8yn+huqWt/tt75qFb/Rclk6SjZ+p7z6TE+++rZcIJ75l/ljX1nnnveHPvGUlqllzsPRMXqgq0L18VQVZNk3SqMt575nSF/6JzQY6vtNz/2NTEf0SSTpf736aTJUneM7FN/Z8PDer5L1KXXv+k94wkNUrwX8gyyPd6WWP/x7ZebLn3jCQ1jfP/vv3Rk7d7z9yytPZtOFMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMCEnHMumg2/1eoH3lf+7oNtvWfijwXr1Jkmlf5D8QEWgiv3P75QPf9jS2lU6j0jSVUu5D0TH+t/fJVV/vdDRWWwxzYmJqqnaJgGSf4LtLkA913ZmQCLpiUEWzQtyL5OnvRfEC8U4P6OCfnPVAZ8PrgTCd4zMaf8H9uEAD+LGr8fbHHJBi/8JdCcr9eqXqx1G84UAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwcdFuWPFxofeVd/q+/wzwRZL6b7ov/OfiTAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYELOOXe5DwIAcGXgTAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYP4P8Qrd51waCvsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at these images, these are reasonable mistakes. For example, when I (a human) look the first image, I see that it is a shoe of some sort. The NN mistaked the sneaker for scandel. I think this is a reasonable error since I myself was not able to conclude what type of footwear the image is depicting. Simiraly, while viewing the following two images, I cannot even tell what either item is, therefore the errors are reasonable."
      ],
      "metadata": {
        "id": "zZbcKpQH7ZM0"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}